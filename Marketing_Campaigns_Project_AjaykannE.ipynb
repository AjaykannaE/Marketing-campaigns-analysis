{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science with Python\n",
    "## Course-End Project: Marketing Campaigns Analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Submitted by:** Ajaykanna E  \n",
    "**Course:** Applied Data Science with Python  \n",
    "**Topic:** Marketing Campaigns — Exploratory Data Analysis & Hypothesis Testing\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The **Marketing Mix** is one of the most widely utilized frameworks in executing marketing strategies. It encompasses the **four Ps of marketing** — Product, Price, Place, and Promotion — each playing a critical role in how businesses acquire and retain customers.\n",
    "\n",
    "In this project, we take the role of a **Data Scientist** and conduct:\n",
    "- **Exploratory Data Analysis (EDA)** to understand customer demographics, spending behavior, and channel preferences\n",
    "- **Hypothesis Testing** to validate key business assumptions\n",
    "- **Visualization-driven Insights** to uncover actionable patterns\n",
    "\n",
    "The dataset captures customer-level information across four dimensions:\n",
    "- **People:** Birth year, education, income, marital status\n",
    "- **Product:** Spending on wines, fruits, meat, fish, sweets, and gold\n",
    "- **Place:** Number of purchases via web, catalog, and store channels\n",
    "- **Promotion:** Responses to five marketing campaigns and deals\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Libraries and Loading Data\n",
    "\n",
    "We begin by importing all necessary libraries for data manipulation, visualization, statistical testing, and machine learning preprocessing. Then, we load the dataset and inspect it to understand its structure, data types, and any immediate quality concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical testing\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, ttest_ind, f_oneway, pointbiserialr\n",
    "\n",
    "# Encoding\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('marketing_data.csv')\n",
    "\n",
    "print(f'Dataset Shape: {df.shape}')\n",
    "print(f'\\nColumn Names:')\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and basic info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine specific columns that need verification\n",
    "print('--- Dt_Customer column (first 10 values) ---')\n",
    "print(df['Dt_Customer'].head(10).tolist())\n",
    "\n",
    "print('\\n--- Income column (raw format, first 10 values) ---')\n",
    "print(df[' Income '].head(10).tolist())\n",
    "\n",
    "print('\\n--- Income column dtype ---')\n",
    "print(df[' Income '].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from Step 1:**\n",
    "\n",
    "- The dataset contains **2,240 rows** and **28 columns**.\n",
    "- The column `Dt_Customer` is imported as an **object (string)** — it needs to be converted to a proper datetime format.\n",
    "- The `Income` column is also imported as a **string** due to formatting issues (dollar signs `$`, commas `,`, and extra whitespace), instead of a numeric type. This needs cleaning.\n",
    "- The column name `' Income '` has **leading and trailing spaces** — it will be renamed for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Data Cleaning and Missing Value Imputation\n",
    "\n",
    "Before analysis, we must clean the dataset thoroughly:\n",
    "- Fix the `Income` column (remove `$`, commas, spaces and convert to float)\n",
    "- Parse `Dt_Customer` as a datetime object\n",
    "- Standardize the `Education` and `Marital_Status` categories (some contain inconsistent or nonsensical values)\n",
    "- Impute missing `Income` values using **group-level medians** (grouped by `Education` and `Marital_Status`), since customers with similar education and marital status tend to have comparable incomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Income column to remove whitespace\n",
    "df.rename(columns={' Income ': 'Income'}, inplace=True)\n",
    "\n",
    "# Fix Income: remove $, commas, and spaces; convert to float\n",
    "df['Income'] = df['Income'].str.replace('$', '', regex=False)\\\n",
    "                           .str.replace(',', '', regex=False)\\\n",
    "                           .str.strip()\\\n",
    "                           .astype(float)\n",
    "\n",
    "print('Income dtype after cleaning:', df['Income'].dtype)\n",
    "print('Sample Income values:')\n",
    "print(df['Income'].head(5).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Dt_Customer to datetime\n",
    "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%m/%d/%y')\n",
    "\n",
    "print('Dt_Customer dtype after conversion:', df['Dt_Customer'].dtype)\n",
    "print('Sample Dt_Customer values:')\n",
    "print(df['Dt_Customer'].head(5).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unique values in categorical columns\n",
    "print('Education unique values:', df['Education'].unique())\n",
    "print('\\nMarital_Status unique values:', df['Marital_Status'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Education column: standardize '2n Cycle' -> 'Master' (equivalent qualification)\n",
    "df['Education'] = df['Education'].replace({'2n Cycle': 'Master'})\n",
    "\n",
    "# Clean Marital_Status: map ambiguous/absurd values to meaningful categories\n",
    "# 'YOLO', 'Alone', 'Absurd' are considered as 'Single'\n",
    "df['Marital_Status'] = df['Marital_Status'].replace({\n",
    "    'YOLO': 'Single',\n",
    "    'Alone': 'Single',\n",
    "    'Absurd': 'Single'\n",
    "})\n",
    "\n",
    "print('Education (cleaned):', df['Education'].unique())\n",
    "print('Marital_Status (cleaned):', df['Marital_Status'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Count': missing, 'Missing %': missing_pct})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing Income using group median (Education + Marital_Status)\n",
    "df['Income'] = df.groupby(['Education', 'Marital_Status'])['Income']\\\n",
    "                 .transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Verify no missing values remain\n",
    "print('Missing values after imputation:')\n",
    "print(df.isnull().sum().sum(), 'total missing values remaining')\n",
    "print(f'\\nIncome — Missing after imputation: {df[\"Income\"].isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from Step 2:**\n",
    "\n",
    "- The `Income` column had **currency formatting** (`$`, commas) that prevented it from being read as numeric — this has been cleaned and cast to float.\n",
    "- `Dt_Customer` was parsed successfully into Python datetime format.\n",
    "- The `Education` column had `'2n Cycle'` which is equivalent to a **Master's level** qualification — this was standardized accordingly.\n",
    "- `Marital_Status` had three non-standard entries — `'YOLO'`, `'Alone'`, and `'Absurd'` — all of which were mapped to `'Single'` since they represent individuals living alone without a partner.\n",
    "- **24 customers** had missing `Income` values (~1.07%). These were imputed using the **group median** based on `Education` and `Marital_Status`, ensuring contextually appropriate estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Feature Engineering\n",
    "\n",
    "We derive new variables that are more analytically useful than the raw columns:\n",
    "- **Total_Children:** Sum of `Kidhome` and `Teenhome`\n",
    "- **Age:** Derived from `Year_Birth` using the reference year 2014 (approximate year of data collection based on `Dt_Customer`)\n",
    "- **Total_Spending:** Sum of all product expenditure columns\n",
    "- **Total_Purchases:** Sum of purchases across all three channels (web, catalog, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of children (kids + teens)\n",
    "df['Total_Children'] = df['Kidhome'] + df['Teenhome']\n",
    "\n",
    "# Age (reference year based on data collection period)\n",
    "reference_year = 2014\n",
    "df['Age'] = reference_year - df['Year_Birth']\n",
    "\n",
    "# Total spending across all product categories\n",
    "product_cols = ['MntWines', 'MntFruits', 'MntMeatProducts',\n",
    "                'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "df['Total_Spending'] = df[product_cols].sum(axis=1)\n",
    "\n",
    "# Total purchases across all channels\n",
    "channel_cols = ['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "df['Total_Purchases'] = df[channel_cols].sum(axis=1)\n",
    "\n",
    "print('New features created:')\n",
    "print(df[['Total_Children', 'Age', 'Total_Spending', 'Total_Purchases']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from Step 3:**\n",
    "\n",
    "- The `Age` feature ranges from very young to extremely old values — this will be examined during outlier detection.\n",
    "- `Total_Spending` captures the overall product expenditure per customer, enabling holistic comparisons across demographics.\n",
    "- `Total_Purchases` aggregates transactions across web, catalog, and store channels — useful for analyzing overall purchasing activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Distribution Analysis and Outlier Treatment\n",
    "\n",
    "We generate **box plots** and **histograms** for key numerical variables to:\n",
    "- Understand the shape and spread of distributions\n",
    "- Identify extreme outliers that could distort statistical analysis\n",
    "\n",
    "Outliers are treated using the **IQR (Interquartile Range)** method — values beyond 1.5×IQR from the quartiles are capped (winsorized) rather than dropped, to preserve data volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key numeric columns for distribution analysis\n",
    "num_cols = ['Age', 'Income', 'Total_Spending', 'Total_Purchases',\n",
    "            'MntWines', 'MntFruits', 'MntMeatProducts',\n",
    "            'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "\n",
    "# --- Histograms ---\n",
    "fig, axes = plt.subplots(2, 5, figsize=(22, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    axes[i].hist(df[col].dropna(), bins=30, color='steelblue', edgecolor='white', alpha=0.8)\n",
    "    axes[i].set_title(f'Distribution of {col}', fontsize=11, fontweight='bold')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Histograms of Key Numerical Features', fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Box Plots ---\n",
    "fig, axes = plt.subplots(2, 5, figsize=(22, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    axes[i].boxplot(df[col].dropna(), patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightcoral', color='darkred'),\n",
    "                    medianprops=dict(color='navy', linewidth=2))\n",
    "    axes[i].set_title(f'Boxplot of {col}', fontsize=11, fontweight='bold')\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "plt.suptitle('Box Plots of Key Numerical Features', fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier treatment using IQR capping (winsorization)\n",
    "def cap_outliers_iqr(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    before = ((df[col] < lower) | (df[col] > upper)).sum()\n",
    "    df[col] = df[col].clip(lower=lower, upper=upper)\n",
    "    return before\n",
    "\n",
    "outlier_cols = ['Age', 'Income', 'Total_Spending', 'Total_Purchases',\n",
    "                'MntWines', 'MntFruits', 'MntMeatProducts',\n",
    "                'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "\n",
    "print('Outlier Treatment Summary (IQR Capping):')\n",
    "print('-' * 40)\n",
    "for col in outlier_cols:\n",
    "    count = cap_outliers_iqr(df, col)\n",
    "    print(f'  {col}: {count} outliers capped')\n",
    "\n",
    "print('\\nDataset shape after treatment:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from Step 4:**\n",
    "\n",
    "- The **Age** distribution revealed a few customers with birth years suggesting ages above 100, which are clearly data entry errors. These were capped using the IQR method.\n",
    "- **Income** showed a right-skewed distribution with a few very high earners pulling the tail — these were also capped.\n",
    "- Spending columns like `MntWines` and `MntMeatProducts` exhibited heavy right skew with notable outliers, which were addressed through winsorization (capping rather than removing, to preserve sample size).\n",
    "- **Total_Spending** and **Total_Purchases** also had high-end outliers, which were treated consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Encoding Categorical Variables\n",
    "\n",
    "Machine learning models and some statistical analyses require numerical input. We apply:\n",
    "- **Ordinal Encoding** for `Education`, which has a natural hierarchical order (Basic < Master < Graduation < PhD)\n",
    "- **One-Hot Encoding** for `Marital_Status` and `Country`, which are nominal (no natural order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encoding for Education\n",
    "education_order = [['Basic', 'Master', 'Graduation', 'PhD']]\n",
    "oe = OrdinalEncoder(categories=education_order)\n",
    "df['Education_Encoded'] = oe.fit_transform(df[['Education']]).astype(int)\n",
    "\n",
    "print('Education Ordinal Encoding:')\n",
    "print(df[['Education', 'Education_Encoded']].drop_duplicates()\n",
    "      .sort_values('Education_Encoded').reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for Marital_Status and Country\n",
    "df = pd.get_dummies(df, columns=['Marital_Status', 'Country'], drop_first=False)\n",
    "\n",
    "# Show newly created dummy columns\n",
    "ohe_cols = [c for c in df.columns if c.startswith('Marital_Status_') or c.startswith('Country_')]\n",
    "print('One-Hot Encoded columns created:')\n",
    "print(ohe_cols)\n",
    "print(f'\\nDataset shape after encoding: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from Step 5:**\n",
    "\n",
    "- `Education` was encoded **ordinally** (0 to 3) reflecting its inherent hierarchy — a PhD is educationally superior to a Basic qualification.\n",
    "- `Marital_Status` and `Country` were encoded using **one-hot encoding**, creating separate binary columns for each category. This prevents the model from incorrectly assuming any ordinal relationship between, say, 'Spain' and 'Germany'.\n",
    "- The dataset expanded from 28 to additional columns due to the new dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Correlation Heatmap\n",
    "\n",
    "A correlation heatmap visualizes the **Pearson correlation coefficients** between all pairs of numeric variables. This helps us identify:\n",
    "- **Strongly correlated features** (multicollinearity risks for modeling)\n",
    "- **Variables most associated with spending, purchases, or campaign acceptance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant numeric columns for correlation\n",
    "core_cols = ['Age', 'Income', 'Total_Children', 'Total_Spending', 'Total_Purchases',\n",
    "             'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
    "             'MntSweetProducts', 'MntGoldProds', 'NumWebPurchases',\n",
    "             'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n",
    "             'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4',\n",
    "             'AcceptedCmp5', 'Response', 'Education_Encoded']\n",
    "\n",
    "corr_matrix = df[core_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdYlGn',\n",
    "    center=0,\n",
    "    vmin=-1, vmax=1,\n",
    "    linewidths=0.5,\n",
    "    square=True,\n",
    "    annot_kws={'size': 8}\n",
    ")\n",
    "plt.title('Correlation Heatmap of Key Variables', fontsize=15, fontweight='bold', pad=15)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "plt.yticks(rotation=0, fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from Step 6:**\n",
    "\n",
    "- **Income and Total_Spending** have a strong positive correlation — higher earners tend to spend significantly more.\n",
    "- **Total_Children** shows a moderate negative correlation with spending — customers with more children tend to spend less, likely due to budget constraints.\n",
    "- **NumStorePurchases and NumCatalogPurchases** are positively correlated with each other, suggesting some customers prefer offline/catalog channels together.\n",
    "- Campaign acceptance rates (**AcceptedCmp1–5, Response**) show moderate mutual correlations, indicating that customers who accept one campaign are somewhat likely to accept others.\n",
    "- **NumWebVisitsMonth** has a weak or slightly negative relationship with purchases, which is explored further in the hypothesis testing section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Hypothesis Testing\n",
    "\n",
    "We formally test four business hypotheses using appropriate statistical tests. Each test follows the standard framework:\n",
    "- **H₀ (Null Hypothesis):** No significant effect or difference exists\n",
    "- **H₁ (Alternative Hypothesis):** A significant effect or difference exists\n",
    "- **Significance level:** α = 0.05\n",
    "\n",
    "We use **p-values** to make decisions: if p < 0.05, we reject H₀."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 1: Older Individuals Prefer In-Store Shopping\n",
    "\n",
    "**Assumption:** Older customers may have lower digital literacy and thus prefer traditional in-store shopping over online channels.\n",
    "\n",
    "**Test:** Pearson correlation between `Age` and `NumStorePurchases`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1: Age vs In-Store Purchases\n",
    "corr1, p1 = pearsonr(df['Age'], df['NumStorePurchases'])\n",
    "\n",
    "print('=== Hypothesis 1: Older Customers Prefer In-Store Shopping ===')\n",
    "print(f'H0: No significant correlation between Age and NumStorePurchases')\n",
    "print(f'H1: Older customers make more in-store purchases')\n",
    "print(f'\\nPearson Correlation (r): {corr1:.4f}')\n",
    "print(f'P-value: {p1:.4f}')\n",
    "print(f'\\nDecision: {\"Reject H0 — Significant correlation found\" if p1 < 0.05 else \"Fail to Reject H0 — No significant correlation\"}')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(df['Age'], df['NumStorePurchases'], alpha=0.4, color='steelblue', s=20)\n",
    "m, b = np.polyfit(df['Age'], df['NumStorePurchases'], 1)\n",
    "axes[0].plot(sorted(df['Age']), [m*x+b for x in sorted(df['Age'])], color='red', linewidth=2)\n",
    "axes[0].set_xlabel('Age', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Store Purchases', fontsize=12)\n",
    "axes[0].set_title(f'Age vs In-Store Purchases\\n(r={corr1:.3f}, p={p1:.4f})', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Also compare with web purchases\n",
    "corr_web, _ = pearsonr(df['Age'], df['NumWebPurchases'])\n",
    "axes[1].scatter(df['Age'], df['NumWebPurchases'], alpha=0.4, color='orange', s=20)\n",
    "m2, b2 = np.polyfit(df['Age'], df['NumWebPurchases'], 1)\n",
    "axes[1].plot(sorted(df['Age']), [m2*x+b2 for x in sorted(df['Age'])], color='red', linewidth=2)\n",
    "axes[1].set_xlabel('Age', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Web Purchases', fontsize=12)\n",
    "axes[1].set_title(f'Age vs Web Purchases\\n(r={corr_web:.3f})', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation — Hypothesis 1:**\n",
    "\n",
    "The Pearson correlation and p-value reveal the statistical relationship between age and in-store shopping frequency. A statistically significant positive correlation supports the notion that older customers lean toward physical retail. The comparison with web purchase correlation helps determine whether the preference is truly directional — i.e., whether older customers actively avoid digital channels or simply visit stores more by habit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 2: Customers with Children Prefer Online Shopping\n",
    "\n",
    "**Assumption:** Parents with children may have time constraints, making online shopping more convenient.\n",
    "\n",
    "**Test:** Independent samples t-test comparing `NumWebPurchases` between customers with and without children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H2: Children vs Web Purchases\n",
    "with_children = df[df['Total_Children'] > 0]['NumWebPurchases']\n",
    "without_children = df[df['Total_Children'] == 0]['NumWebPurchases']\n",
    "\n",
    "t2, p2 = ttest_ind(with_children, without_children, equal_var=False)\n",
    "\n",
    "print('=== Hypothesis 2: Customers with Children Prefer Web Shopping ===')\n",
    "print(f'H0: No significant difference in web purchases between parents and non-parents')\n",
    "print(f'H1: Customers with children make more web purchases')\n",
    "print(f'\\nMean Web Purchases (with children):    {with_children.mean():.3f}')\n",
    "print(f'Mean Web Purchases (without children): {without_children.mean():.3f}')\n",
    "print(f'T-statistic: {t2:.4f}')\n",
    "print(f'P-value: {p2:.4f}')\n",
    "print(f'\\nDecision: {\"Reject H0 — Significant difference found\" if p2 < 0.05 else \"Fail to Reject H0 — No significant difference\"}')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].boxplot([without_children, with_children],\n",
    "                labels=['No Children', 'Has Children'],\n",
    "                patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue'))\n",
    "axes[0].set_title('Web Purchases: Parents vs Non-Parents', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Web Purchases')\n",
    "\n",
    "plot_data = pd.DataFrame({\n",
    "    'Group': ['No Children'] * len(without_children) + ['Has Children'] * len(with_children),\n",
    "    'Web Purchases': pd.concat([without_children, with_children]).values\n",
    "})\n",
    "axes[1].bar(['No Children', 'Has Children'],\n",
    "            [without_children.mean(), with_children.mean()],\n",
    "            color=['steelblue', 'coral'], edgecolor='black', alpha=0.8)\n",
    "axes[1].set_title('Average Web Purchases by Parental Status', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Mean Web Purchases')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation — Hypothesis 2:**\n",
    "\n",
    "The Welch's t-test (which does not assume equal variances) compares mean web purchases between customers with and without children. If p < 0.05 and the mean is higher for customers with children, it supports the hypothesis that time constraints push parents toward online shopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 3: In-Store Sales Are Cannibalized by Other Channels\n",
    "\n",
    "**Assumption:** As web and catalog purchases increase, in-store purchases might decrease — suggesting channel competition.\n",
    "\n",
    "**Test:** Pearson correlation between `NumStorePurchases` and both `NumWebPurchases` and `NumCatalogPurchases`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H3: Channel Cannibalization\n",
    "corr_store_web, p_sw = pearsonr(df['NumStorePurchases'], df['NumWebPurchases'])\n",
    "corr_store_cat, p_sc = pearsonr(df['NumStorePurchases'], df['NumCatalogPurchases'])\n",
    "\n",
    "print('=== Hypothesis 3: In-Store Sales Cannibalized by Other Channels ===')\n",
    "print(f'H0: No negative correlation between in-store and other channel purchases')\n",
    "print(f'H1: Increased web/catalog purchases reduce in-store purchases')\n",
    "print(f'\\nCorrelation (Store vs Web):     r = {corr_store_web:.4f}, p = {p_sw:.4f}')\n",
    "print(f'Correlation (Store vs Catalog): r = {corr_store_cat:.4f}, p = {p_sc:.4f}')\n",
    "\n",
    "for label, corr, p in [('Web', corr_store_web, p_sw), ('Catalog', corr_store_cat, p_sc)]:\n",
    "    if corr < 0 and p < 0.05:\n",
    "        print(f'\\n{label}: Reject H0 — Significant negative correlation (cannibalization supported)')\n",
    "    else:\n",
    "        print(f'\\n{label}: Fail to Reject H0 — No negative correlation (no cannibalization)')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(df['NumWebPurchases'], df['NumStorePurchases'], alpha=0.3, color='teal', s=20)\n",
    "axes[0].set_xlabel('Web Purchases')\n",
    "axes[0].set_ylabel('Store Purchases')\n",
    "axes[0].set_title(f'Store vs Web Purchases\\n(r={corr_store_web:.3f}, p={p_sw:.4f})', fontweight='bold')\n",
    "\n",
    "axes[1].scatter(df['NumCatalogPurchases'], df['NumStorePurchases'], alpha=0.3, color='purple', s=20)\n",
    "axes[1].set_xlabel('Catalog Purchases')\n",
    "axes[1].set_ylabel('Store Purchases')\n",
    "axes[1].set_title(f'Store vs Catalog Purchases\\n(r={corr_store_cat:.3f}, p={p_sc:.4f})', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation — Hypothesis 3:**\n",
    "\n",
    "Cannibalization would be indicated by a **negative** correlation between in-store and alternative channel purchases. A positive correlation, on the other hand, would suggest that heavy buyers purchase across all channels — meaning these channels complement rather than compete with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 4: Does the USA Significantly Outperform Other Countries in Total Purchases?\n",
    "\n",
    "**Assumption:** The United States may dominate total purchase volumes due to market size, spending power, or campaign effectiveness.\n",
    "\n",
    "**Test:** Independent samples t-test comparing `Total_Purchases` for US customers vs. rest of the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need the original Country column — re-derive it from dummies or check source\n",
    "# Reconstruct Country label from one-hot columns\n",
    "country_cols = [c for c in df.columns if c.startswith('Country_')]\n",
    "df['Country_Label'] = df[country_cols].idxmax(axis=1).str.replace('Country_', '')\n",
    "\n",
    "us_purchases = df[df['Country_Label'] == 'US']['Total_Purchases']\n",
    "non_us_purchases = df[df['Country_Label'] != 'US']['Total_Purchases']\n",
    "\n",
    "t4, p4 = ttest_ind(us_purchases, non_us_purchases, equal_var=False)\n",
    "\n",
    "print('=== Hypothesis 4: USA vs Rest of World in Total Purchases ===')\n",
    "print(f'H0: No significant difference between USA and rest of the world purchase volumes')\n",
    "print(f'H1: USA significantly outperforms other countries in total purchases')\n",
    "print(f'\\nMean Total Purchases (US):           {us_purchases.mean():.3f}  (n={len(us_purchases)})')\n",
    "print(f'Mean Total Purchases (Rest of World): {non_us_purchases.mean():.3f}  (n={len(non_us_purchases)})')\n",
    "print(f'T-statistic: {t4:.4f}')\n",
    "print(f'P-value: {p4:.4f}')\n",
    "print(f'\\nDecision: {\"Reject H0 — USA significantly outperforms\" if p4 < 0.05 and us_purchases.mean() > non_us_purchases.mean() else \"Fail to Reject H0 — No significant outperformance\"}')\n",
    "\n",
    "# Visualization — average purchases by country\n",
    "country_avg = df.groupby('Country_Label')['Total_Purchases'].mean().sort_values(ascending=False)\n",
    "\n",
    "colors = ['coral' if c == 'US' else 'steelblue' for c in country_avg.index]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(country_avg.index, country_avg.values, color=colors, edgecolor='black', alpha=0.85)\n",
    "plt.title('Average Total Purchases by Country (US highlighted)', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Mean Total Purchases')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation — Hypothesis 4:**\n",
    "\n",
    "The t-test compares the mean total purchases of US customers against all others combined. The bar chart provides an additional visual breakdown by country. If the US mean is higher and statistically significant (p < 0.05), the hypothesis is supported — otherwise, it suggests that other geographies are equally competitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Business Insights through Visualization\n",
    "\n",
    "In this final section, we use targeted visualizations to answer five key business questions about product performance, customer behavior, and campaign effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Top-Performing and Lowest-Revenue Products\n",
    "\n",
    "We compare the total revenue generated by each product category to identify the company's best and worst performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_revenue = df[product_cols].sum().sort_values(ascending=False)\n",
    "product_labels = {\n",
    "    'MntWines': 'Wines',\n",
    "    'MntMeatProducts': 'Meat',\n",
    "    'MntGoldProds': 'Gold',\n",
    "    'MntFishProducts': 'Fish',\n",
    "    'MntSweetProducts': 'Sweets',\n",
    "    'MntFruits': 'Fruits'\n",
    "}\n",
    "product_revenue.index = [product_labels.get(i, i) for i in product_revenue.index]\n",
    "\n",
    "colors = ['#2ecc71' if i < 2 else '#e74c3c' if i >= len(product_revenue)-2 else '#3498db'\n",
    "          for i in range(len(product_revenue))]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "bars = axes[0].bar(product_revenue.index, product_revenue.values, color=colors, edgecolor='black', alpha=0.85)\n",
    "axes[0].set_title('Total Revenue by Product Category', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Total Revenue ($)')\n",
    "axes[0].set_xlabel('Product Category')\n",
    "for bar in bars:\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000,\n",
    "                 f'${bar.get_height():,.0f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "axes[1].pie(product_revenue.values, labels=product_revenue.index,\n",
    "            autopct='%1.1f%%', startangle=140,\n",
    "            colors=['#2ecc71', '#27ae60', '#3498db', '#e67e22', '#e74c3c', '#c0392b'])\n",
    "axes[1].set_title('Revenue Share by Product Category', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Top Product:    {product_revenue.index[0]} — ${product_revenue.iloc[0]:,.0f}')\n",
    "print(f'Lowest Product: {product_revenue.index[-1]} — ${product_revenue.iloc[-1]:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations — 8a:**\n",
    "\n",
    "**Wines** is the dominant product category by a significant margin, contributing the largest share of total revenue. **Meat products** rank second. At the bottom, **Fruits** and **Sweets** generate the least revenue, suggesting these categories may need targeted promotional strategies or better placement to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Age vs Last Campaign Acceptance Rate\n",
    "\n",
    "We examine whether a customer's age is associated with their likelihood of accepting the most recent campaign (`Response`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age groups\n",
    "bins = [0, 35, 50, 65, 200]\n",
    "labels_age = ['Under 35', '35–50', '50–65', 'Above 65']\n",
    "df['Age_Group'] = pd.cut(df['Age'], bins=bins, labels=labels_age)\n",
    "\n",
    "campaign_by_age = df.groupby('Age_Group', observed=True)['Response'].agg(['mean', 'count']).reset_index()\n",
    "campaign_by_age.columns = ['Age_Group', 'Acceptance_Rate', 'Count']\n",
    "campaign_by_age['Acceptance_Rate_Pct'] = campaign_by_age['Acceptance_Rate'] * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(campaign_by_age['Age_Group'], campaign_by_age['Acceptance_Rate_Pct'],\n",
    "            color=['#1abc9c', '#3498db', '#9b59b6', '#e74c3c'], edgecolor='black', alpha=0.85)\n",
    "axes[0].set_title('Last Campaign Acceptance Rate by Age Group', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Acceptance Rate (%)')\n",
    "axes[0].set_xlabel('Age Group')\n",
    "for i, row in campaign_by_age.iterrows():\n",
    "    axes[0].text(i, row['Acceptance_Rate_Pct'] + 0.3,\n",
    "                 f\"{row['Acceptance_Rate_Pct']:.1f}%\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "axes[1].scatter(df['Age'], df['Response'], alpha=0.15, color='steelblue', s=15)\n",
    "corr_ar, p_ar = pointbiserialr(df['Age'], df['Response'])\n",
    "axes[1].set_title(f'Age vs Campaign Response\\n(Point-Biserial r = {corr_ar:.3f}, p = {p_ar:.4f})', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].set_ylabel('Accepted Last Campaign (1=Yes, 0=No)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(campaign_by_age.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations — 8b:**\n",
    "\n",
    "The bar chart reveals how campaign acceptance rates differ across age groups. Alongside the point-biserial correlation, this helps us understand whether age meaningfully predicts campaign responsiveness. If younger or older customers show distinctly different acceptance rates, campaign targeting can be tailored accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Country with Highest Last Campaign Acceptance\n",
    "\n",
    "We determine which country has the highest proportion of customers who accepted the last campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_campaign = df.groupby('Country_Label')['Response'].agg(['sum', 'mean', 'count']).reset_index()\n",
    "country_campaign.columns = ['Country', 'Accepted_Count', 'Acceptance_Rate', 'Total_Customers']\n",
    "country_campaign['Acceptance_Rate_Pct'] = country_campaign['Acceptance_Rate'] * 100\n",
    "country_campaign = country_campaign.sort_values('Acceptance_Rate_Pct', ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "top_country = country_campaign.iloc[0]['Country']\n",
    "colors = ['#e74c3c' if c == top_country else '#3498db' for c in country_campaign['Country']]\n",
    "\n",
    "axes[0].bar(country_campaign['Country'], country_campaign['Acceptance_Rate_Pct'],\n",
    "            color=colors, edgecolor='black', alpha=0.85)\n",
    "axes[0].set_title('Last Campaign Acceptance Rate by Country', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Acceptance Rate (%)')\n",
    "axes[0].set_xlabel('Country')\n",
    "for i, row in country_campaign.reset_index().iterrows():\n",
    "    axes[0].text(i, row['Acceptance_Rate_Pct'] + 0.3,\n",
    "                 f\"{row['Acceptance_Rate_Pct']:.1f}%\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "axes[1].bar(country_campaign['Country'], country_campaign['Accepted_Count'],\n",
    "            color=colors, edgecolor='black', alpha=0.85)\n",
    "axes[1].set_title('Total Customers Accepted Last Campaign by Country', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of Customers Accepted')\n",
    "axes[1].set_xlabel('Country')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Campaign Acceptance by Country:')\n",
    "print(country_campaign.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations — 8c:**\n",
    "\n",
    "The chart highlights which country has the highest campaign acceptance rate (highlighted in red). This can guide the marketing team in identifying geographies where campaigns are resonating most effectively, and where similar strategies could be replicated or amplified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8d. Number of Children at Home vs Total Expenditure\n",
    "\n",
    "We investigate whether having more children at home is associated with lower or higher total spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spending_by_children = df.groupby('Total_Children')['Total_Spending'].agg(['mean', 'median', 'count']).reset_index()\n",
    "spending_by_children.columns = ['Total_Children', 'Mean_Spending', 'Median_Spending', 'Count']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(spending_by_children['Total_Children'].astype(str),\n",
    "            spending_by_children['Mean_Spending'],\n",
    "            color='#2ecc71', edgecolor='black', alpha=0.85)\n",
    "axes[0].set_title('Mean Total Spending by Number of Children', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Total Number of Children (Kids + Teens)')\n",
    "axes[0].set_ylabel('Mean Total Spending ($)')\n",
    "for i, row in spending_by_children.iterrows():\n",
    "    axes[0].text(i, row['Mean_Spending'] + 5, f\"${row['Mean_Spending']:.0f}\",\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Box plot\n",
    "groups = [df[df['Total_Children'] == c]['Total_Spending'].values for c in sorted(df['Total_Children'].unique())]\n",
    "bp = axes[1].boxplot(groups, patch_artist=True,\n",
    "                     labels=[str(c) for c in sorted(df['Total_Children'].unique())],\n",
    "                     boxprops=dict(facecolor='lightgreen'),\n",
    "                     medianprops=dict(color='navy', linewidth=2))\n",
    "axes[1].set_title('Total Spending Distribution by Number of Children', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Total Number of Children')\n",
    "axes[1].set_ylabel('Total Spending ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(spending_by_children.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations — 8d:**\n",
    "\n",
    "A clear pattern emerges: customers with **zero children** spend significantly more than those with one or more children. As the number of children increases, average spending tends to decline — likely reflecting the competing financial demands of raising children. This suggests that **child-free customers** represent a higher-value segment worth targeting with premium product offerings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8e. Educational Background of Customers Who Lodged Complaints\n",
    "\n",
    "We analyze whether customers who filed complaints in the last two years have a distinct educational background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Education column from original data — re-derive from Education_Encoded\n",
    "edu_map = {0: 'Basic', 1: 'Master', 2: 'Graduation', 3: 'PhD'}\n",
    "df['Education_Label'] = df['Education_Encoded'].map(edu_map)\n",
    "\n",
    "complainers = df[df['Complain'] == 1]\n",
    "non_complainers = df[df['Complain'] == 0]\n",
    "\n",
    "complain_edu = complainers['Education_Label'].value_counts()\n",
    "all_edu = df['Education_Label'].value_counts()\n",
    "complain_rate = (complainers['Education_Label'].value_counts() / all_edu * 100).sort_values(ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].bar(complain_edu.index, complain_edu.values, color='#e74c3c', edgecolor='black', alpha=0.85)\n",
    "axes[0].set_title('Number of Complainers by Education', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count of Complaints')\n",
    "axes[0].set_xlabel('Education Level')\n",
    "\n",
    "axes[1].bar(complain_rate.index, complain_rate.values, color='#e67e22', edgecolor='black', alpha=0.85)\n",
    "axes[1].set_title('Complaint Rate (%) by Education Level', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Complaint Rate (%)')\n",
    "axes[1].set_xlabel('Education Level')\n",
    "for i, (edu, rate) in enumerate(complain_rate.items()):\n",
    "    axes[1].text(i, rate + 0.05, f'{rate:.2f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "edu_order = ['Basic', 'Master', 'Graduation', 'PhD']\n",
    "complain_pivot = df.groupby('Education_Label')['Complain'].mean() * 100\n",
    "complain_pivot = complain_pivot.reindex(edu_order, fill_value=0)\n",
    "axes[2].plot(edu_order, complain_pivot.values, marker='o', linewidth=2, color='purple', markersize=8)\n",
    "axes[2].fill_between(edu_order, complain_pivot.values, alpha=0.2, color='purple')\n",
    "axes[2].set_title('Complaint Rate by Education (Ordered)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Complaint Rate (%)')\n",
    "axes[2].set_xlabel('Education Level')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Complaint Statistics by Education:')\n",
    "print(df.groupby('Education_Label')['Complain'].agg(['sum', 'mean']).rename(\n",
    "    columns={'sum': 'Total Complaints', 'mean': 'Complaint Rate'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations — 8e:**\n",
    "\n",
    "The complaint analysis across education levels reveals which customer groups are most likely to raise concerns. Graduated and higher-educated customers may have higher expectations, which could explain different complaint patterns. This insight can help the customer service team develop targeted resolution and communication strategies for different education segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary of Key Findings\n",
    "\n",
    "| Area | Key Insight |\n",
    "|------|-------------|\n",
    "| **Data Quality** | Income had currency formatting issues and 24 missing values; Marital Status had 3 invalid categories |\n",
    "| **Feature Engineering** | Age, Total_Children, Total_Spending, Total_Purchases were derived for richer analysis |\n",
    "| **Correlations** | Income strongly drives Total_Spending; Children negatively correlate with spending |\n",
    "| **H1 — Age & In-Store** | Test reveals whether older customers statistically prefer store over web |\n",
    "| **H2 — Children & Web** | Parents may or may not make significantly more web purchases than non-parents |\n",
    "| **H3 — Cannibalization** | Correlation direction indicates whether channels compete or complement each other |\n",
    "| **H4 — US Market** | Statistical test reveals whether US customers significantly outpace global peers |\n",
    "| **Top Product** | Wines dominates product revenue; Fruits and Sweets are lowest performers |\n",
    "| **Children & Spending** | Child-free customers spend significantly more — prime target for premium products |\n",
    "| **Campaigns** | Age groups and countries show varying campaign acceptance rates |\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project demonstrates a complete **end-to-end data science workflow** applied to a real-world marketing dataset:\n",
    "\n",
    "1. **Data Import & Verification** — Confirmed and corrected data type issues for `Income` and `Dt_Customer`\n",
    "2. **Data Cleaning** — Standardized categorical variables and applied group-based median imputation\n",
    "3. **Feature Engineering** — Created meaningful derived features for richer analysis\n",
    "4. **Distribution Analysis** — Used histograms, box plots, and IQR-based outlier capping\n",
    "5. **Encoding** — Applied ordinal encoding for ordered categories and one-hot encoding for nominal ones\n",
    "6. **Correlation Analysis** — Heatmap revealed key relationships between demographics, spending, and campaign metrics\n",
    "7. **Hypothesis Testing** — Four business hypotheses tested using Pearson correlation and Welch's t-test\n",
    "8. **Business Visualizations** — Five targeted analyses provided actionable insights on products, campaigns, and customer behavior\n",
    "\n",
    "These findings can directly inform marketing strategy: identifying which channels to invest in, which customer segments to prioritize, and where campaign efforts yield the greatest return.\n",
    "\n",
    "---\n",
    "*Project by Ajaykanna E | Applied Data Science with Python | Simplilearn*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
